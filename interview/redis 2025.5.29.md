### 1. 跳表他是线程安全的吗？详细讲讲跳表是怎么实现的？跳表的查询复杂度是多少？怎么计算出的是logn
#### 1.1 跳表的实现机制
跳表的实现机制是它本质上是一种多层链表结构。
1. **分层结构：**
    - **Level 0 (最底层)：** 包含所有数据节点，它们按照排序键（例如分数和成员）严格有序排列，形成一个基本的双向链表。
    - **更高层级 (索引层)：** 在 Level 0 之上，通过随机选择的方式，将一部分节点“提升”到更高的层级。每一层都是其下一层的“稀疏子集”，就像地图上的不同缩放级别。这种分层结构的核心在于，随着层数的升高，链表中的节点数量呈几何级数减少。
2. **随机层数：**
    - 当插入一个新节点时，会使用一个**随机函数**（基于一个预设的概率 P，例如 Redis 使用 P=0.25）来决定这个新节点将存在于多少个层级中。这个概率决定了节点被提升到更高层的可能性。
    - 这种概率性的层级分配，使得跳表的结构在平均情况下保持平衡，从而保证了其高效的性能。Redis 原生的实现当中最大层数一共有 32 层。
3. **核心操作：**
    - **`update` 数组的作用：** 在插入或删除一个节点时，算法需要从跳表的最高层开始，逐层向下查找目标位置。在这个查找过程中，**`update` 数组**扮演着**修改路径记录器**的角色。具体来说，`update[i]` 会记录在 Level `i` 上，新节点应该插入到哪个节点的**后面**（或者说，待删除节点的前一个节点）。当查找完成，`update` 数组就包含了所有受影响层级的**前驱节点**的引用。这样，在实际进行指针和 `span` 值修改时，可以直接通过 `update` 数组访问到需要修改的节点，避免了重复查找，极大地提高了写入效率。
    - **`span` 值的含义与更新：** `span` 值是跳表的一个重要优化，它存储在每个节点的每个层级中。它代表了从当前节点到它在当前层 `forward` 指向的下一个节点之间，在**最底层（Level 0）上实际跳过了多少个元素**。在插入和删除节点时，`span` 值需要被精确地更新。当一个节点被插入或删除，其前驱节点和后继节点在相关层级的 `span` 值都可能需要调整，以反映 Level 0 上元素数量的变化。这个调整确保了排名查询的准确性。
4. **跳跃查询：** 查找操作从跳表的最高层开始。在当前层，我们沿着 `forward` 指针向右移动，直到找到一个节点的 `score` 大于目标值，或者在当前层无法再向前移动。此时，我们就下降到下一层，并从当前位置继续查找。这个过程重复进行，直到到达 Level 0。由于每一层节点的稀疏性，我们可以在高层“跳过”大量的节点，从而迅速缩小搜索范围，最终在 Level 0 精确定位到目标节点。
#### 1.2 跳表的线程安全与并发编程
在并发环境下，多个线程同时对跳表进行 `insert` 或 `delete` 操作时，会存在显著的竞态条件。
1. **并发挑战（竞态条件）：**
    - **写操作的原子性问题：** 例如，`size` 变量的增减并非原子操作，可能导致计数错误。更关键的是，多线程并发修改 `SkipListNode` 内部的 `level` 数组中的 `forward` 指针和 `span` 值时，可能出现：
        - **丢失更新：** 一个线程对 `update[i].level[i].forward` 的修改，可能被另一个线程的并发操作覆盖，导致跳表结构损坏。
        - **数据不一致：** `span` 值的计算和更新依赖于前一步的 `traversed` 和 `rank` 信息，并发修改可能导致 `span` 计算错误，进而影响排名查询的准确性。
    - **读写冲突：** 即使是读操作（如 `getElementByRankRange` 或 `getElementByScoreRange`），如果与写操作并发进行，也可能读到处于中间状态的跳表结构，导致数据不一致或 NullPointerException。
2. **Redis 的解决方案：** Redis 采用的是**单线程模型**来处理所有命令请求，这是一个根本性的设计决策。所有对跳表的操作都在主线程中顺序执行，杜绝了并发访问带来的竞态条件。从 Redis 6.0 开始引入的多线程只是用于 I/O 读写，并不涉及数据结构的并发操作，核心命令执行依然是单线程。这体现了 Redis 对简单性、高性能和强一致性的权衡，它将并发控制的复杂性推向了客户端。
3. **如何使跳表线程安全（加锁策略与粒度）：**
    要使跳表在多线程环境中安全运行，必须引入并发控制机制。
    - **读写锁 (ReadWriteLock) 的选择：** 对于跳表这种读操作可能多于写操作的数据结构，**读写锁（如 Java 中的 `ReentrantReadWriteLock`）是比普通互斥锁更好的选择**。它允许多个读线程同时访问，但在有写线程访问时，所有读写线程都会被阻塞。这提升了读操作的并发性，适用于读多写少的场景。
    - **加锁的力度：** 加锁的力度应该设置为**跳表的整个实例**，而不是单个节点。原因在于，跳表的插入和删除操作本质上会修改**多层**的多个节点指针和 `span` 值。这些修改是一个逻辑上的“原子操作单元”，需要一起完成以保持跳表结构的完整性。如果只对单个节点加锁，无法保证整个跳表结构在并发修改下的一致性。因此，锁的粒度至少要覆盖**一次完整的逻辑操作**（如一次 `insert` 或 `delete`）。
    - **更细粒度的并发（无锁）：** 虽然读写锁提供了很好的平衡，但要实现**极致的并发性能**，则需要采用**无锁编程**。 CA直接修改指针，
#### 1.3 跳表的查询复杂度及其 O(logN) 推导
跳表的平均查询复杂度是 **O(logN)**，其中 N 是跳表中元素的总数量。
**O(logN) 的原理推导：**
1. **层数与对数关系：** 假设节点晋升到更高一层的概率为 P。那么，拥有 k 层（即存在于 Level k−1 但不存在于 Level k）的节点的概率是 Pk−1(1−P)。对于一个包含 N 个节点的跳表，其最高层数 L 的**期望值**大约为 log1/P​N。例如，如果 P=0.25，那么最高层数约为 log4​N。这意味着层数随着节点数量的增加呈对数增长。
2. **单层查找的效率：** 在跳表的每一层，我们进行横向查找。由于每隔 1/P 个 Level `i` 的节点，就会有一个 Level `i+1` 的节点，因此在每一层上，我们平均只需要进行**常数次**（大约 1/P 次）的横向跳跃，就能到达下一个更高层级的节点。
3. **总的查询路径：** 查询的总路径由**垂直下降的层数**和**每层横向移动的步数**组成。垂直下降的层数是 O(L)，即 O(logN)。每层横向移动的期望步数是常数。因此，总的查找操作次数大约是 (1/P)×log1/P​N，由于 1/P 是一个常数，所以整体查询复杂度为 **O(logN)**。
**`span` 值如何辅助实现 O(logN) 查询（特别是按排名查询）：**
`span` 值在**按排名查询**（如查找第 k 个元素）时，是实现 O(logN) 效率的关键。在进行按排名查询时，我们从最高层开始，维护一个累积的“已跳过元素数量”计数器。在每一层，我们可以利用当前节点 `forward` 指针的 `span` 值，直接将计数器加上这个 `span` 值，从而判断沿着这条路径前进是否会越过目标排名。这使得我们能够在高层进行“大步跳跃”，一次性跳过大量 Level 0 的元素，而不需要逐个遍历。这种高效的机制，结合跳表的对数层级，使得按排名查询也能在 **O(logN)** 时间内完成。

### 2. 跳表如何变成线程安全的呢？怎么样加锁最好以保证他为线程安全的，加锁的力度应该设置为多少？加读写锁怎么样呢？
跳表本身并不是线程安全的。在 Redis 中，通过**单线程模型**来处理所有命令，从而规避了并发问题，避免了对跳表进行并发控制。
如果要在多线程 Java 环境中实现线程安全的跳表，可以采用以下策略：
- **读写锁**：这是相对较好的加锁方式。
    - **读锁**：允许多个线程同时持有读锁进行并发查询，只要没有写操作，它们之间不会阻塞。
    - **写锁**：一次只允许一个线程持有写锁进行插入或删除操作，写锁会阻塞所有读写操作，确保数据一致性。
- **加锁粒度**：
    - **在操作方法级别加锁**：应该在跳表的**核心操作方法**（如 `put`、`get`、`remove`）上加读写锁。`get` 方法获取读锁，而 `put` 和 `remove` 方法获取写锁。
    - **不推荐节点级别加锁**：不建议在跳表的每个节点上加锁。这样做会带来极高的维护开销，并且由于跳表多层结构和指针操作的复杂性，难以正确地维护锁的原子性和避免死锁。
- **Java 源码中的实现**：Java 的 `ConcurrentSkipListMap` 并没有使用传统的读写锁，而是采用了更高级的**无锁算法**。它通过**原子引用**和 **CAS** 操作来实现线程安全。这种方式避免了锁的开销和潜在的死锁问题，从而提供了更高的并发性能。


### 3. 什么情况下会使用二分，数据一定要有序吗？
二分查找主要用于在**有序集合**中查找特定元素。
数据**不一定需要全局有序，但必须具备某种形式的单调性**。
- **完全有序**是最理想的情况。
- 对于**部分有序**的结构，例如**旋转有序数组**（`[4, 5, 6, 7, 0, 1, 2]`），可以通过**修改二分查找的逻辑**来适应其内部的有序性，实现 O(logn) 的查找效率。

### 4. Redis 当中还有一个数据结构叫布隆过滤器，他的具体原理是什么，用途是什么？误判率可以通过什么方式来降低呢？听过多次哈希函数吗？
**布隆过滤器**的原理是使用一个**很长的二进制位数组**和**多个哈希函数**。
- **添加元素**：通过 `k` 个哈希函数计算得到 `k` 个哈希值，并将位数组中对应这 `k` 个位置的位设置为 `1`。
- **查询元素**：如果所有对应的 `k` 个位**全部为 `1`**，则元素**可能存在**（存在误判）；若**任一位置为 `0`**，则元素**一定不存在**。
其主要**用途**是**快速判断元素是否存在**，例如，可以用来解决**缓存穿透**问题。
**降低误判率**的方式有几种：
- **增加位数组的长度**：位数组越长，位冲突的概率越低。
- **增加哈希函数的数量**：哈希函数数量的增加也能在一定程度上降低误判率。

### 5. 你刚才还聊到一个 ZSet，它的结构能大体说一下吗，底层实现用了什么数据结构？
Redis 的 **ZSet** 是一种有序集合，它将每个**成员（member）**与一个**分数（score）**进行关联。ZSet 中的每个元素都是一个 **score-member 对**，并根据分数进行排序。
它的底层实现是一个**复合结构**：
- **压缩列表 (ziplist)**： 当 ZSet 存储的元素数量较少且成员和分数都比较小的时候，Redis 会使用 `ziplist` 作为底层实现。`ziplist` 是一种紧凑的双向链表，在这种模式下，**score 和 member 是紧挨着存储的**，以节省内存。
- **跳表 (skiplist) + 字典 (dict)**： 一旦 ZSet 的元素数量或单个成员/分数的大小**达到预设阈值**（由配置项决定），它会自动从 `ziplist` 转换为 **`skiplist` + `dict`** 的组合结构。
    - **跳表**负责根据分数维护元素的有序性，支持高效的范围查询和排名操作。
    - **字典 (dict)**：这是一个**哈希表**，用于维护 **member 到 score 的映射**。在 `ZADD` 操作时，除了将元素添加到跳表以保持有序，也会同步更新这个 `dict`。这样，就能方便地使用像 `ZSCORE` 这样的命令，以 O(1) 的时间复杂度快速查询指定成员的分数。 这个 `dict` 尤其值得一提的是，它采用了 **渐进式哈希（incremental rehashing）** 机制。在 Redis 的哈希表扩容时，它会在内部维护两张表，当一张表达到阈值后，会在每次 `CRUD`（增删改查）操作过程中，**分批次地将数据从旧表迁移到容量更大的新表上**，最终完成所有数据的迁移，从而避免了因一次性大量数据拷贝而导致的阻塞。

### 6. 假如有一个业务需要做一个排行榜的话使用 ZSet 合适吗？如果是一个实时排行榜只显示前几个，但是等到结束之后会拉取整个榜单，实时排行榜只有几个值，这个值该从 Redis 还是 MySQL 中获取呢？
使用 Redis 的 **ZSet** 来实现排行榜**非常合适**。
- **ZSet 的优势**：
    - 它是**天生有序**的数据结构，能够自动根据分数进行排序。
    - 底层结合跳表和哈希表实现，支持 O(logN) 的插入、删除和范围查询，同时能以 O(1) 的时间复杂度查询指定成员的分数。
    - 提供了 `ZRANGE` 等便捷命令，非常方便地从 Redis 中获取特定范围的数据。
- **实时排行榜（只显示前几个）的数据来源**：
    - **直接从 Redis 获取**。因为 ZSet 能够高效地获取前几名数据，满足实时性要求，延迟极低。
- **结束后的完整榜单数据来源**：
    - **从 Redis 查询**。我认为，除非数据量达到**超级巨大**（例如，数十亿级别且超出了 Redis 的内存承载能力），否则都应该从 Redis 中查询。Redis ZSet 即使在数据量很大的情况下，查询速度依然保持在 O(logN) 的级别，这远快于传统数据库的查询，并且避免了跨系统查询的复杂性和额外开销。
    - 如果数据规模真的达到了 Redis 无法承载的程度，那么才需要考虑将历史或全量榜单数据持久化到 MySQL 或其他分布式存储中。

### 7. 缓存和 MySQL 的交互中保证数据一致性主流使用哪些方式？

在缓存和 MySQL 交互中，保证数据一致性需要根据具体的业务场景和对一致性强度的要求来选择不同的策略。
#### 1. 读多写少场景（通用业务场景）
- **策略**：**先更新数据库，再删除缓存**。
- **优点**：
    - 性能较好：读操作主要走 Redis 缓存，写操作只涉及删除缓存，开销小。
- **一致性问题与应对**：
    - **短暂不一致窗口**：在数据库更新完成到缓存被删除之间，如果恰好有读请求，可能会读到旧数据。
    - **补偿机制**：
        - 为删除操作添加**重试机制**。
        - 使用**消息队列 + 延迟双删**：更新数据库后发送消息到消息队列，消费者延时（如几秒）再次尝试删除缓存，确保缓存最终失效。
#### 2. 写多读少或强一致性要求场景
- **策略**：基于**消息队列的最终一致性 + 锁**。
- **优点**：
    - 最终一致性：保证缓存和数据库数据最终一致。
    - 解耦：数据库和缓存同步逻辑分离，异步处理。
- **实现方式**：
    - 业务操作**只写入 MySQL**。
    - 通过订阅 MySQL 的 **Binlog**（或使用 CDC 工具），将数据库变更事件实时发送到消息队列。
    - 缓存服务作为消费者，订阅消息队列中的变更，异步更新或删除缓存。
- **一致性保障**：
    - 对于需要保证严格一致性的场景（如库存扣减、交易），还需配合业务层面的**数据库事务**、**乐观锁（版本号）**或**悲观锁**来确保数据操作的原子性和正确性。

### 8. 如果让你设计一个全球性排行榜，数据量在几千万，如果让你设计这个系统，你应该从哪些方面来考虑？首先是不是要使用集群呢？按照什么来进行数据分片比较好一点按照什么来分片，按国家还是按时区呢？只用分布式缓存吗，需要本地缓存来存吗？本地缓存和分布式缓存对比应用场景是什么？

设计一个几千万数据量的全球性排行榜，我的核心思路是结合 Redis **ZSet** 的优势，并利用**集群**来保证高可用性和可扩展性。
#### 1. 系统设计考虑与集群使用
- **数据结构**：排行榜的核心当然是 Redis 的 **ZSet**，它天生有序，非常适合此类场景。
- **集群是必须的**。单机 Redis 无法支撑几千万的数据量和全球性的高并发读写请求，内存和并发能力都会成为瓶颈。使用 Redis 集群可以有效分散数据和请求压力，并提供高可用性。
#### 2. 数据分片策略
分片策略需要根据业务访问模式来定。我的思路是：
- **按国家/时区进行分片**：将每个国家或主要时区的排行榜 ZSet 存储在不同的分片或集群中。例如，`zset:leaderboard:us`, `zset:leaderboard:jp` 等。
    - **优势**：绝大多数用户会查询自己所属区域的榜单，这样请求会被路由到单一分片，减少跨分片查询的复杂性。
- **全球总榜**：由于按区域分片后，直接查询全球榜会很困难（需要聚合所有分片）。因此，需要**额外维护一个独立的全球总榜 ZSet**。
    - **实时性**：如果全球总榜需要实时性，每次用户分数更新时，除了更新对应区域榜，也同步更新全球总榜。这个全球总榜可能需要独立的分片或集群来承载。
    - **非实时性**：如果全球总榜可以接受一定延迟，可以设置后台任务，例如**每天定时**聚合所有区域榜的数据，计算并更新全球总榜。这种方式在展示时可直接读取预计算好的总榜。
#### 3. 缓存层级与应用场景
- **分布式缓存（Redis Cluster）**：
    - **核心层**。全球排行榜的实时数据必须存储在分布式缓存中，它是主要的读写数据源，提供高性能和高可用性。它承载着数千万的排行榜数据。
- **本地缓存**：
    - **是需要的，作为分布式缓存的补充**。在应用服务层引入本地缓存（如 Caffeine, Guava Cache）可以进一步提升性能，减少对分布式缓存的访问压力。
    - **应用场景**：主要用于缓存**超高频访问的少量热点数据**，比如排行榜的前几名（TOP 10-100）。这些数据虽然会变化，但查询量极大，将其缓存在应用服务的本地内存中可以大幅降低网络延迟和 Redis 集群的负载。
    - **缓存策略**：通常采用**短时过期**（例如，几秒或几十秒）或 **LRU** 淘汰策略。
    - **一致性处理**：本地缓存与分布式缓存之间的一致性通常通过**短时过期**来保证最终一致。或者，当分布式缓存数据更新时，可以设计机制**通知本地缓存失效**。

### 9. 这种多级缓存之间的一致性可以采用哪种？使用版本号如何？ 结合业务场景和具体情况实际说说
对于多级缓存（本地缓存 + 分布式缓存 + 数据库）之间的一致性，在排行榜这种业务场景下，我们通常追求的是**最终一致性**，而非强一致性。因为排行榜允许短时间的数据不一致，而且过度的强一致性会牺牲性能。
几种常见的一致性策略及其在排行榜场景的应用：
#### 1. **过期策略（TTL）**
- **实现方式**：为本地缓存和分布式缓存都设置合理的过期时间（Time To Live）。
- **业务场景应用**：
    - **排行榜场景**：最常用且有效的方式。例如，本地缓存可以设置 5-10 秒的过期时间，分布式缓存（Redis ZSet）可以视为数据源本身，没有硬性过期时间，或者根据业务规则设置排行榜的周期性过期。
    - **具体实践**：实时排行榜的前几名数据（如 TOP 10）可以在本地缓存中设置非常短的 TTL（例如 5 秒），因为它更新频繁且查询量大。用户在 5 秒内看到的可能是略微过时的数据，但对于排行榜来说通常是可接受的。
- **优点**：实现简单，维护成本低。
- **缺点**：在过期时间内数据可能不一致，无法保证实时一致性。
#### 2. **被动更新/失效策略（Cache Aside 模式的延伸）**
- **实现方式**：当底层数据源（MySQL）或更上层缓存（分布式缓存）数据发生变化时，主动通知下层缓存失效或更新。
    - **分布式缓存到本地缓存**：当 Redis ZSet 中的排行榜数据发生更新时（例如，玩家分数改变导致排名变化），可以通过**消息队列**或 **Redis 的 Pub/Sub 机制**通知所有相关的应用服务实例。应用服务收到通知后，主动删除或更新其本地缓存中的对应排行榜数据。
    - **MySQL 到分布式缓存**：这部分已经在问题 7 中讨论过，通常采用**先更新数据库，再删除分布式缓存**的模式，或者通过 **Binlog 订阅**来保证分布式缓存的最终一致性。
- **业务场景应用**：
    - 排行榜分数更新时，`ZADD` 操作成功后，可以发布一个消息到消息队列，所有订阅了该消息的应用服务实例收到后，清除本地缓存中对应的排行榜数据。下次请求时，应用服务会从 Redis 重新加载最新数据到本地缓存。
- **优点**：数据一致性比纯过期策略好，能够更快地反映数据变化。
- **缺点**：实现复杂度增加，需要额外的通知机制。
#### 3. **版本号（Version Control）**

- **实现方式**：在数据记录中增加一个版本号字段。
    - **读操作**：从缓存中获取数据时，也获取其版本号。
    - **写操作**：更新数据时，将版本号递增。然后尝试更新缓存。在更新缓存时，可以带上版本号作为条件，只有当缓存中的版本号与当前数据库中的版本号一致时才更新，否则表示缓存已过期或被其他线程更新过，需要重新从数据库拉取。
    - **缓存同步**：在分布式缓存（如 Redis）中，可以将版本号与数据一起存储。当数据更新时，版本号也递增。在本地缓存中，也可以缓存数据及其版本号。
- **业务场景应用**：
    - **排行榜场景**：ZSet 并没有直接的版本号字段。但可以为整个排行榜（或者排行榜的某个分区）维护一个**全局版本号**。例如，每当排行榜数据发生变化时，更新一个 Redis Key `leaderboard:version` 的值。
    - **具体实践**：
        1. 客户端或应用服务读取排行榜时，首先从 Redis 读取 `leaderboard:version`。
        2. 然后读取 ZSet 数据。
        3. 将版本号和 ZSet 数据一起放入本地缓存。
        4. 下次读取时，先比较本地缓存中的版本号和 Redis 中的 `leaderboard:version`。如果不一致，说明 Redis ZSet 有更新，本地缓存失效，需要重新从 Redis 拉取最新 ZSet 数据和版本号。
- **优点**：
    - 能有效解决“脏读”问题，保证读取到最新的数据。
    - 比纯过期策略提供更强的一致性。
- **缺点**：
    - **复杂度较高**：需要在数据模型和业务逻辑中引入版本号管理。
    - **额外开销**：每次读写都需要处理版本号。
    - **排行榜的特殊性**：ZSet 本身是集合，每次单个元素分数变化可能导致整个集合的排序变化，如果每次变化都更新一个全局版本号，这个版本号的更新频率会非常高，可能成为新的热点。所以更适合于粒度较大的数据。


### 10. 我们给 Redis 加数据，他会不会淘汰？他的淘汰策略是什么样的？说说分布式锁，他来处理多级缓存会不会比较好。
是的，当我们给 Redis 加数据时，如果内存达到上限，它**会淘汰**旧数据。
#### Redis 淘汰策略
Redis 提供了多种淘汰策略，通过配置 `maxmemory-policy` 来决定：
- **LRU (Least Recently Used)**：最近最少使用。
- **LFU (Least Frequently Used)**：最不经常使用。
- **TTL (Time To Live)**：淘汰剩余寿命最短（即将过期）的键。
- **随机淘汰**：随机选择键淘汰。
- **`noeviction`**：当内存不足时，新写入操作报错，不淘汰任何数据。
#### 分布式锁与多级缓存一致性
分布式锁**不适合**用来处理多级缓存之间的一致性，尤其是在**排行榜这种读多写少**的场景。
- **不适合原因**：性能开销大、实现复杂度高，且不符合排行榜这种最终一致性场景的需求。
- **合适应用场景**：它主要用于在分布式环境下**保证共享资源的互斥访问**，例如防止重复支付、高并发库存扣减等。
### 11. 哈希结构你能讲讲吗，他的结构是什么样的？Redis 里面的哈希表是会加读写锁的吗？
Redis 中的哈希结构是其内部 **`dict`** 的具体应用。
- **结构特点**：
    - **冲突解决**：`dict` 采用**链式地址法**来解决哈希冲突，即每个哈希桶（bucket）会维护一个链表，存储所有哈希到该位置的键值对。
    - **渐进式哈希**：`dict` 在扩容或缩容时，会采用**渐进式哈希**机制。这意味着，它不会一次性地进行 `rehash`（将所有数据从旧表迁移到新表），而是在每次增、删、改、查操作过程中，**逐步**进行 `rehash`，实现一种**非阻塞式**的扩容迁移。
- **是否加读写锁**：
    - **不加读写锁**。Redis 自身是**单线程模型**。所有命令的执行都是串行化的，因此在任何给定时间，只有一个命令会操作哈希表，天然保证了哈希表操作的原子性和线程安全，无需额外的锁机制。
### 12. 你了解 Geo 吗？做地图用 Geo 比较好一点吗？从技术上来说比如说京东外卖技术上存在什么问题，定位会不准吗？
是的，我了解 Redis 的 **Geo** 数据结构。
#### Redis Geo 结构及其优势
Redis 的 Geo 是专门用于存储地理位置信息（经度、纬度）的数据结构，并支持基于位置的距离计算和范围查找。
- **底层实现**：Geo 是基于 **ZSet** 实现的。
    - ZSet 中的 **score** 存储的是**地理哈希（Geohash）**编码后的 52 位整数值。
    - **Geohash** 将二维经纬度编码成一维值，其关键特性是：编码越长，精度越高；相同前缀的 Geohash 编码代表的地理位置越接近。这使得 Geo 可以利用 ZSet 的范围查询能力来实现附近的人/商店功能。
- **做地图相关应用，Geo 比较好吗？**
    - **是的，从技术上来说，Redis Geo 在处理“附近的人/地点”、“范围查询”和“距离计算”等地图相关功能时表现非常出色。**它提供高性能的 O(logN) 或 O(logN+M) 级别的查询，且功能丰富。
#### 京东外卖类应用中的 Geo 技术问题与定位不准分析
对于京东外卖这类需要精确地理定位的应用，虽然 Geo 提供了强大支持，但仍可能存在技术挑战和定位不准的问题：
1. **Geohash 自身的精度与边界问题（Geo 技术特性）**：
    - **精度损失**：Geohash 是对地理空间的离散化。**编码越长，精度越高；编码越短，精度越低**。如果追求极高的精确度，ZSet 的 score 会变得非常密集，这可能影响 ZSet 性能，但在多数场景下 Redis Geo 已作优化。
    - **边界问题**：处于不同 Geohash 网格边界的相邻点，它们的 Geohash 编码可能相差较大。Redis 的 `GEORADIUS` / `GEOSEARCH` 命令在内部已经考虑了这个问题，会自动查询周围的多个 Geohash 区域来确保完整性。


### 13. Redis 的 BigKey 和 HotKey 能聊一下吗？定义是什么，他所带来的问题是什么？怎么解决呢？有没有了解过什么检测 HotKey 的框架？
是的，**BigKey** 和 **HotKey** 是 Redis 使用中两个关键的概念，它们对服务性能和稳定性影响很大。它们的“大”和“热”是根据具体的业务场景和系统负载来定义的，并没有绝对的数值。
#### 1. BigKey (大 Key)

- **定义**：指存储了**过大值**的键。例如，一个 String 类型的键存储了 MB 甚至 GB 级别的数据，或者一个 List、Hash、Set、ZSet 等集合类型存储了**非常多的成员**（如几十万或几百万个元素）。
- **带来的问题**：
    - **阻塞持久化**：在进行 RDB 持久化或 AOF 重写时，处理 BigKey 会导致 Redis 主进程长时间阻塞。
    - **占用网络 I/O 资源**：读写 BigKey 会产生大量网络流量，瞬间占满带宽，影响其他命令的响应速度。
    - **内存碎片**：删除 BigKey 后，释放的大块内存可能无法被操作系统有效回收和利用，导致内存碎片化。
    - **主从复制延迟**：主节点删除 BigKey 后，同步给从节点的操作也会引起较大的延迟。
- **解决方案**：
    - **拆分 BigKey**：将一个大键拆分成多个小键。例如，一个包含百万用户的 Set 可以拆分成多个小 Set，每个 Set 只存储部分用户。
    - **异步删除**：使用 Redis 4.0 引入的 **`UNLINK` 命令**替代 `DEL` 命令，它能将删除操作异步化，避免阻塞主线程。
    - **监控预警**：结合工具（如 `redis-cli --bigkeys`）定期扫描和预警 BigKey。
#### 2. HotKey (热 Key)
- **定义**：指在**短时间内被高频访问**的键。例如，一个热门商品的信息键、一个突发新闻的访问计数器。
- **带来的问题**：
    - **单节点瓶颈**：所有对 HotKey 的请求都集中到 Redis 集群的**同一个节点**上，导致该节点 CPU 飙升、网络带宽打满，进而影响该节点上其他键的正常访问。
    - **缓存击穿/雪崩风险**：HotKey 短暂失效或被删除时，大量请求会瞬间打到后端数据库，可能导致数据库压力过大甚至崩溃。
- **解决方案**：
    - **读写分离**：增加从节点，将 HotKey 的读请求分散到多个从节点上。
    - **本地缓存**：在应用服务层引入**本地缓存**（如 Java 中的 Caffeine 或 Guava Cache），存储 HotKey 的数据。这样，超高频的请求可以直接从应用本地内存获取，大大减轻 Redis 的压力。
    - **数据打散**：如果业务允许，可以将 HotKey 拆分为多个带有随机后缀的小键（例如 `key:rand(0~N)`），将请求分散到不同的键上，降低单个键的热度。
- **检测 HotKey 的框架**：
    - **Redis 自身工具**：可以使用 `redis-cli --hotkeys` 命令直接扫描 Redis 实例，发现访问频率较高的键。
### 14. Redisson 的框架看过吗，能讲讲他的底层实现吗？
Redisson 的底层实现主要基于以下核心技术：
- **高性能网络通信**： Redisson 底层基于 **Netty** 实现，通过**非阻塞 I/O** (NIO) 与 Redis 服务端进行通信。这使得单个线程能够处理大量并发请求，显著提升了吞吐量。同时，Redisson 内部包含**高效的连接池**，复用与 Redis 的连接，减少了开销。
- **分布式对象与 Lua 脚本**： Redisson 封装了 Redis 的基本数据结构为 Java 友好的**分布式对象**（如 `RMap`, `RLock`）。它的许多复杂操作（如分布式锁的加锁、释放，限流等）都通过**Lua 脚本**来保证**原子性**，因为 Redis 保证 Lua 脚本的原子执行。
- **分布式锁实现 (`RLock`)**： Redisson 提供了强大的分布式锁功能。
    - 它支持 **RedLock 算法**，这是一种高可用、强一致性的分布式锁方案，需要在多个 Redis Master 节点上同步获取锁。
    - 对于单 Redis 实例或主从模式，其锁机制基于 `SET key value NX PX expiration_ms` 命令，并结合 Lua 脚本原子性地释放锁。
    - 关键的 **“看门狗”（Watchdog）机制**：当客户端持有锁的业务执行时间超过锁的过期时间时，看门狗会自动为锁进行**续期**，防止锁因业务逻辑耗时过长而提前释放。
- **集群与拓扑支持**： Redisson 能够很好地支持 Redis Cluster、Sentinel、Master-Slave 等多种部署模式。它能自动发现集群拓扑变化，并根据 Key 的 Slot 槽位信息，将请求路由到正确的节点。


### 15. 超卖和少卖问题，超卖是什么原因导致的？应该用户下单之后扣减库存还是支付成功扣减库存呢？下单成功扣减库存有什么缺点呢？实际上会把这二者结合起来，下单后预扣减，用 Redis 去做这部分。

**超卖**是指商品的实际库存量少于系统显示的库存量，导致卖出的商品超出真实库存。
- **超卖原因**：
    - **并发竞争**：多个用户同时请求购买同一商品，读取到的是旧库存，导致扣减请求数量超过实际库存。
    - **缓存与数据库不一致**：缓存中的库存信息未及时更新，导致用户看到的库存量大于实际可用库存。
**少卖**是指商品明明有库存，但因系统显示售罄而无法售出。
- **少卖原因**：
    - **库存扣减过于严格**：库存被锁定但未能及时回滚（例如，用户下单后未支付或取消订单），导致库存长时间被占用。
#### 库存扣减时机与优缺点
在选择下单后扣减还是支付成功后扣减时，需要权衡利弊：
- **下单后扣减库存（预扣减）**：
    - **优点**：能有效**避免超卖**，用户下单后即锁定库存，提升用户体验。
    - **缺点**：库存管理复杂，可能导致**少卖**。若用户未支付或取消订单，被锁定的库存需及时回滚，否则会显示商品售罄。
- **支付成功后扣减库存**：
    - **优点**：能减少少卖风险，只有真实支付成功才扣减库存。
    - **缺点**：**超卖风险较高**。在高并发下，多个用户可能同时支付，导致最终扣减量大于实际库存，需要更强的并发控制。
#### 实际业务中如何结合：Redis 预扣减
在实际高并发业务中，通常会**结合两种方式，并利用 Redis 进行预扣减**：
1. **下单阶段进行 Redis 预扣减**：
    - 用户下单时，利用 Redis 的**原子操作**（如 `DECRBY`）对库存进行预扣减。
    - 由于 Redis 是单线程执行命令，`DECRBY` 操作本身是原子性的，这天然解决了高并发下的超卖问题。
    - 预扣减成功后，可以将订单状态标记为“待支付”。
    - 如果 `DECRBY` 返回的库存值小于 0，说明库存不足，需回滚 Redis 计数器（`INCRBY`）并提示用户下单失败。
    - **分布式锁**（如 Redisson 的 `RLock`）也可以用于保障对库存键的逻辑操作互斥性，但主要依赖 `DECRBY` 的原子性。
2. **支付成功后正式确认库存**：
    - 用户支付成功，订单状态更新为“已支付”，此时库存正式扣减成功。
    - 若支付失败、超时未支付或取消订单，则通过**异步机制**（如延迟队列或消息队列）触发库存**回滚**，将预扣减的库存加回 Redis 和数据库。
3. **最终异步同步至 MySQL**
    - Redis 承担高并发预扣减的任务，而最终的库存变动（支付成功或回滚）会**异步同步到 MySQL 数据库**，保证数据的持久化和最终一致性。